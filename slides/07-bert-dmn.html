<!-- bert-dmn -->
<section class="sec-title" id="bert-dmn">
  <h2>
    <b>Sentence-level representations</b><br />for passage ranking
  </h2>
  <div class="pub">
    <p>Jurek Leonhardt, Fabian Beringer, and Avishek Anand</p>
    <p>
      <span class="pub-title">Exploiting Sentence-Level Representations for Passage
        Ranking</span>, <span class="pub-venue">LWDA 2021 (workshop paper)</span>
    </p>
  </div>
</section>

<section id="bert-dmn-motivation">
  <h2>Cross-attention re-rankers</h2>
  <hr class="divider" />

  <div class="column-layout">
    <div class="column-item">
      <img class="fig" src="img/background/cross_encoder_out.svg" />
      <p class="caption">LLM-based cross-attention ranker.</p>
    </div>
    <div class="column-item">
      <div class="c-wide">
        <p>
          Cross-attention re-rankers (e.g., BERT) allow for
          <b>query-document attention</b>, which can be helpful for
          <b>complex queries</b> (e.g., question answering):
        </p>
        <p class="fragment" style="text-align: center" data-fragment-index="1">
          $\phi_{\operatorname{BERT}}(q, d) =$
          <span class="fragment highlight-red" data-fragment-index="2">
            $\operatorname{BERT}_\texttt{[CLS]}$
          </span>
          $(\texttt{[CLS]}\ q\ \texttt{[SEP]}\ d\ \texttt{[SEP]})$
        </p>
        <hr class="divider" />
        <p class="fragment" data-fragment-index="2">
          Often times, only the
          <span class="fragment highlight-red" data-fragment-index="2">
            $\texttt{[CLS]}$ output
          </span>
          is used, and other outputs are discarded.
        </p>
        <hr class="divider" />
        <p class="fragment" data-fragment-index="3">
          <b>Can we utilize the contextualized sentence
            representations that BERT outputs?</b>
        </p>
      </div>
    </div>
  </div>
</section>

<section id="bert-dmn-dmns">
  <h2>Dynamic memory networks</h2>
  <hr class="divider" />
  <p>
    <i>Dynamic memory networks</i><sup>1</sup> (DMNs) were proposed to
    enable <b>cross-sentence reasoning</b> for question answering.
  </p>
  <hr class="divider" />

  <p class="fragment" data-fragment-index="1">
    DMNs operate on <b>all query and document tokens</b>:
  </p>
  <p class="fragment" data-fragment-index="1">
    $\phi_{\operatorname{DMN}}(q, d) = \operatorname{DMN}\left(q_1,
    \dots, q_{|q|}, d_1, \dots, d_{|d|}\right)$,
  </p>
  <p class="fragment" data-fragment-index="1">
    where $q_i$ and $d_i$ are <b>word embeddings</b>.
  </p>
  <hr class="divider" />

  <p class="fragment">
    <b>What if we used contextual BERT representations instead?</b>
  </p>
  <hr class="divider" />
  <p class="footnote">
    <sup>1</sup>Ankit Kumar et al. "Ask Me Anything: Dynamic Memory
    Networks for Natural Language Processing". PMLR 2016.
  </p>
</section>

<section id="bert-dmn-architecture" data-auto-animate data-auto-animate-id="bdmn">
  <h2>BERT-DMN</h2>
  <hr class="divider" />
  <img class="fig" src="img/bert_dmn/architecture_out.svg" />
  <p class="caption">BERT-DMN architechture.</p>
</section>

<section id="bert-dmn-lite-architecture" data-auto-animate data-auto-animate-id="bdmn">
  <h2>BERT-DMN<sub>lite</sub></h2>
  <hr class="divider" />
  <img class="fig" src="img/bert_dmn/lite_architecture_out.svg" />
  <p class="caption">BERT-DMN<sub>lite</sub> architechture.</p>
</section>

<section id="bert-dmn-performance">
  <h2>Ranking performance</h2>
  <hr class="divider" />

  <div class="column-layout" style="grid-template-columns: auto 60%">
    <div class="column-item">
      <div class="c-wide">
        <p class="fragment" data-fragment-index="1">
          BERT-DMN shows improvements over BERT.
        </p>
        <hr class="divider" />
        <p class="fragment" data-fragment-index="2">
          BERT-DMN<sub>lite</sub> performance is comparable to BERT.
        </p>
      </div>
    </div>

    <!-- styling here is required by Safari -->
    <div class="column-item" style="display: grid; height: 100%">
      <div class="tab r-stack" id="bdmn-result-table">
        <img class="fragment fade-out" data-fragment-index="1" src="img/bert_dmn/results/ranking_out.svg" />
        <img class="fragment fade-in-then-out" data-fragment-index="1"
          src="img/bert_dmn/results/ranking_1_out.svg" />
        <img class="fragment fade-in" data-fragment-index="2" src="img/bert_dmn/results/ranking_2_out.svg" />
      </div>
      <p class="caption">Performance on passage ranking datasets.</p>
    </div>
  </div>
</section>

<section id="bert-dmn-training-efficiency">
  <h2>Training efficiency</h2>
  <hr class="divider" />

  <div class="column-layout">
    <div class="column-item">
      <img class="tab" src="img/bert_dmn/results/training_efficiency_out.svg" />
      <p class="caption">Training batches per second.</p>
    </div>
    <div class="column-item">
      <div class="c-wide">
        <p>
          BERT-DMN<sub>lite</sub> reduces the
          <b>number of trainable parameters</b> roughly
          <b>from 110M to 3M</b>.
        </p>
        <hr class="divider" />
        <p>
          For each query-document pair, <b>BERT outputs</b> can be
          <b>cached</b> after the first epoch.
        </p>
      </div>
    </div>
  </div>
</section>

<section id="bert-dmn-summary">
  <h2>Summary</h2>
  <hr class="divider" />
  <ul>
    <li>
      We introduced BERT-DMN and BERT-DMN<sub>lite</sub> for
      re-ranking.
    </li>
    <li>
      BERT-DMN shows that sentence-level BERT representations hold
      useful information for ranking.
    </li>
    <li>
      BERT-DMN<sub>lite</sub> shows that fine-tuning BERT itself is
      not strictly necessary. This can improve training efficiency.
    </li>
  </ul>
  <hr class="divider" />
  <div class="github-container">
    <img class="github-qr" src="img/bert_dmn/qr_github.svg" />
    <a class="github-url" href="https://github.com/mrjleo/ranking-models">mrjleo/ranking-models</a>
  </div>
</section>
<!-- end: bert-dmn --> 